{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make necessary requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pprint import pprint\n",
    "import json\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://www.linkedin.com/uas/login-submit\"\n",
    "redirectURL = \"Place the profile URL\"\n",
    "userEmail = \"Place your email here\"\n",
    "page404 = \"https://www.linkedin.com/jsnfkljn/?_l=en_US\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get login page and csrf value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webSession = requests.Session()\n",
    "headers = {\n",
    "\"User-Agent\":\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/110.0\"\n",
    "}\n",
    "webSession.headers.update(headers)\n",
    "loginPage = webSession.post(baseURL)\n",
    "loginSoup = bs(loginPage.content, \"html.parser\")\n",
    "# pprint(soup.prettify())\n",
    "\n",
    "csrf = (loginSoup.find('input', attrs={'name': 'loginCsrfParam'}))['value']\n",
    "pprint(csrf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the csrf and username and password to make an authenticated POST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_dict = {'session_key':userEmail,'session_password': 'Enter your password here','loginCsrfParam': csrf,'session_redirect': redirectURL}\n",
    "loginSuccessPage = webSession.post(baseURL, data=payload_dict)\n",
    "\n",
    "loginSuccessSoup = bs(loginSuccessPage.content, \"html.parser\")\n",
    "pprint(payload_dict)\n",
    "webSession.get(redirectURL)\n",
    "# pprint(loginSuccessSoup.text)\n",
    "pprint(webSession.cookies.get_dict())\n",
    "\n",
    "# pprint(profileSoup.prettify())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# End existing sessions\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# options = ChromeOptions()\n",
    "options = FirefoxOptions()\n",
    "options.add_argument('--lang=en')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "# service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "service = FirefoxService(executable_path=GeckoDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "# driver.close()\n",
    "# driver.implicitly_wait(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer session over to Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(page404)\n",
    "authCookies = ['li_at', 'liap', 'leadgen.api_session', 'transaction_state', 'lihc_auth_str',\\\n",
    "    'lihc_auth_', 'li_a', 'li_ep_auth_context', 'sj_sessionid', 'cp_session', 'G_AUTHUSER_H',\\\n",
    "        'G_ENABLED_IDPS', 'g_state', 'x-ms-cpim-cache|<id>', 'x-ms-cpim-trans',\\\n",
    "        'x-ms-cpim-sso:linkedinenterprise.onmicrosoft.com_0', 'lidc', 'li_rm']\n",
    "\n",
    "reqCookies = webSession.cookies.get_dict()\n",
    "# pprint(reqCookies)\n",
    "for cookie in reqCookies:\n",
    "    # if(cookie not in authCookies): continue\n",
    "    # pprint({\n",
    "    #     'name': cookie, \n",
    "    #     'value': reqCookies[cookie],\n",
    "    # })\n",
    "    driver.add_cookie({\n",
    "        'name': cookie, \n",
    "        'value': reqCookies[cookie],\n",
    "    })\n",
    "\n",
    "## Copy paste cookies from browser\n",
    "    \n",
    "# pprint(driver.get_cookies())\n",
    "# WebDriverWait(driver,5).until(EC.presence_of_element_located(By.XPATH('//button[text()=\"Some text\"]')))\n",
    "WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page not found')]\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scape the profile!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the profile into the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_initialised(driver):\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    # pprint(soup.prettify())\n",
    "    try:\n",
    "        profileSection = soup.find('div', {'class':'mt2 relative'})\n",
    "        aboutSection = soup.find('div', {'id':'about'})\n",
    "        # pprint(profileSection)\n",
    "        if(profileSection==None or aboutSection==None):\n",
    "            raise Exception('None')\n",
    "        # profileSection = profileSection['pv-text-details__left-panel']\n",
    "    \n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "driver.get(\"https://www.linkedin.com/in/palathingal/\")\n",
    "try:\n",
    "    WebDriverWait(driver, timeout=5).until(document_initialised)\n",
    "except:\n",
    "    pass\n",
    "# driver.switch_to.new_window('tab');\n",
    "# try:\n",
    "# except:\n",
    "#     pass\n",
    "    # driver.get(redirectURL)\n",
    "profileSoup = bs(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileScraped={\n",
    "    'name':\"\",\n",
    "    'titleDescription':\"\",\n",
    "    'location': \"\",\n",
    "    'about': \"\",\n",
    "    'experiences': \"\",\n",
    "    'education': \"\",\n",
    "    'recommendations':\"\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the fist ember (always the profile)\n",
    "# Go down the path for the profile name\n",
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .h1\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "print(res)\n",
    "profileScraped['name'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape the Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .find_all('div', recursive=False)[1]\\\n",
    "        .span\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "print(res)\n",
    "profileScraped['location'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the Title's Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the fist ember (always the profile)\n",
    "# Go down the path for the profile name\n",
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .div\\\n",
    "        .find_all('div', recursive=False)[1]\\\n",
    "        .text\\\n",
    "        .strip()\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "print(res)\n",
    "profileScraped['titleDescription'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the About section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the ember with a child with id='about'\n",
    "# Go down the path for the description\n",
    "def aboutFilter(elem):\n",
    "    # if(elem.find('div',{'id': 'about'})): pprint(elem)\n",
    "    try:\n",
    "        return elem.name=='section' and elem.find('div',{'id': 'about'})\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    res = profileSoup.find(aboutFilter)\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .span\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "print(res)\n",
    "profileScraped['about'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Experience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to the 'Experience' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def experience_initialised(driver):\n",
    "#     driver.\n",
    "driver.get(redirectURL+'details/experience/')\n",
    "try:\n",
    "    WebDriverWait(driver, timeout=5).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\"))\n",
    "except:\n",
    "    pass\n",
    "expSoup = bs(driver.page_source, 'html.parser')\n",
    "expSoup = expSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experience = []\n",
    "pastExperience = []\n",
    "try:\n",
    "    pastExperience = expSoup.find_all('li', recursive=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for exp in pastExperience:\n",
    "    job = {}\n",
    "    subjobs=[]\n",
    "    try:\n",
    "        subjobs = exp.div.div.find_all('div', recursive=False)[1].find_all('div', recursive=False)[1].ul.li.div.div.div.ul.find_all('li', recursive=False)\n",
    "    except:\n",
    "        pass\n",
    "    # pprint(subjobs[0])\n",
    "    if(len(subjobs)!=0):\n",
    "        for sj in subjobs:\n",
    "            job={}\n",
    "            try:\n",
    "                job['company'] = exp.div\\\n",
    "                            .div\\\n",
    "                            .find_all('div', recursive=False)[1]\\\n",
    "                            .div\\\n",
    "                            .a\\\n",
    "                            .div\\\n",
    "                            .span\\\n",
    "                            .span\\\n",
    "                            .text\n",
    "            except:\n",
    "                job['company'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['location'] = exp.div\\\n",
    "                            .div\\\n",
    "                            .find_all('div', recursive=False)[1]\\\n",
    "                            .div\\\n",
    "                            .a\\\n",
    "                            .find('span', recursive=False, attrs={'class': 't-black--light'})\\\n",
    "                            .span\\\n",
    "                            .text\n",
    "            except:\n",
    "                try:\n",
    "                    job['location'] = sj.div\\\n",
    "                        .div\\\n",
    "                        .find_all('div', recursive=False)[1]\\\n",
    "                        .div\\\n",
    "                        .a\\\n",
    "                        .find_all('span', recursive=False, attrs={'class': 't-black--light'})[1]\\\n",
    "                        .span\\\n",
    "                        .text\n",
    "                except:\n",
    "                    job['location'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['position'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .div\\\n",
    "                    .a\\\n",
    "                    .div\\\n",
    "                    .span\\\n",
    "                    .span\\\n",
    "                    .text\n",
    "            except:\n",
    "                job['position'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['dateRange'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .div\\\n",
    "                    .a\\\n",
    "                    .find('span', recursive=False, attrs={'class': 't-black--light'})\\\n",
    "                    .span\\\n",
    "                    .text\n",
    "            except:\n",
    "                job['dateRange'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['description'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .ul\\\n",
    "                    .li\\\n",
    "                    .text\\\n",
    "                    .strip()\n",
    "            except:\n",
    "                job['description'] = 'N/A'\n",
    "                \n",
    "            experience+=[copy.deepcopy(job)]\n",
    "\n",
    "        continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        job['position'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['position'] = 'N/A'\n",
    "    try:\n",
    "        job['company'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find('span', recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split('·')[0]\n",
    "    except:\n",
    "        job['company'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        job['job'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find('span', recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split('·')[1]\n",
    "    except:\n",
    "        job['job'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        job['dateRange'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find_all('span', recursive=False)[1]\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['dateRange'] = 'N/A'\n",
    "            \n",
    "    try:\n",
    "        job['location'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find_all('span', recursive=False)[2]\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['location'] = 'N/A'\n",
    "\n",
    "    try:\n",
    "        job['description'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .ul\\\n",
    "            .li\\\n",
    "            .div\\\n",
    "            .ul\\\n",
    "            .li\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['description'] = 'N/A'\n",
    "            \n",
    "    experience+=[job]\n",
    "pprint(experience)\n",
    "profileScraped['experience'] = experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Education"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to the 'Education' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(redirectURL+'details/education/')\n",
    "try:\n",
    "    WebDriverWait(driver, timeout=7).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\") and d.find_element(By.CLASS_NAME,\"ivm-image-view-model\"))\n",
    "except:\n",
    "    pass\n",
    "edSoup = bs(driver.page_source, 'html.parser')\n",
    "edSoup = edSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Education!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = []\n",
    "pastEducation = []\n",
    "try:\n",
    "    pastEducation = edSoup.find_all('li', recursive=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for current in pastEducation:\n",
    "    school = {}\n",
    "    try:\n",
    "        school['name'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        school['name'] = 'N/A'\n",
    "    try:\n",
    "        school['degree'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span',class_=\"t-14 t-normal\", recursive=False)\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        school['degree'] = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        school['dateRange'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span', {'class': 't-black--light'}, recursive=False)\\\n",
    "            .span\\\n",
    "            .text\n",
    "        \n",
    "    except:\n",
    "        school['dateRange'] = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        school['description'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .text\\\n",
    "        \n",
    "        school['description'] = (re.sub(r'\\n\\s*\\n', '\\n\\n', school['description'])).strip()\n",
    "        \n",
    "    except:\n",
    "        school['description'] = 'N/A'\n",
    "    \n",
    "    education+=[school]\n",
    "\n",
    "print(education)\n",
    "profileScraped['education'] = education"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to Recommendations page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(redirectURL+'details/recommendations/')\n",
    "WebDriverWait(driver, timeout=7).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\") and d.find_element(By.CLASS_NAME,\"t-black--light\"))\n",
    "recSoup = bs(driver.page_source, 'html.parser')\n",
    "recSoup = recSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "pastRecommendations = recSoup.find_all('li', recursive=False)\n",
    "\n",
    "for rec in pastRecommendations:\n",
    "    recLi = {}\n",
    "    try:\n",
    "        recLi['name'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        recLi['name'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        recLi['profile'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .get('href')\n",
    "    except:\n",
    "        recLi['profile'] = 'N/A'\n",
    "\n",
    "    try:\n",
    "        subtitle = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span', attrs = {'class': 't-black--light'}, recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split()\n",
    "        \n",
    "        recLi['date'] = ' '.join(subtitle[:3])\n",
    "        recLi['relationship'] = ' '.join(subtitle[3:])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        recLi['date'] = 'N/A'\n",
    "        recLi['relationship'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        recLi['testimonial'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find('div', attrs={'class': 't-normal'})\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        recLi['testimonial'] = 'N/A'\n",
    "    \n",
    "    recommendations+=[recLi]\n",
    "\n",
    "pprint(recommendations)\n",
    "profileScraped['recommendations'] = recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person\n",
    "person = Person(\"Profile URL\", driver=driver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Voyager API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://www.linkedin.com/voyager/api\"\n",
    "authURL = \"https://www.linkedin.com/uas/authenticate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  linkedin_api import Linkedin\n",
    "api = Linkedin('LinkedIn Email', 'LinkedIn Password')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = api.get_profile('Profile Tag (what comes after linkedin.com/in/....)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newprof = {\n",
    "    'name': ' '.join((profile.get('firstName', ''), profile.get('lastName', ''))),\n",
    "    'titleDescription': profile.get('headline', 'N/A'),\n",
    "    'location': ', '.join((profile.get('geoLocationName', ''), profile.get('geoCountryName', ''))),\n",
    "    'about': profile.get('summary', 'N/A'),\n",
    "}\n",
    "newprof['experiences'] = []\n",
    "for experience in profile['experience']:\n",
    "    newExp = {\n",
    "        'position': experience.get('title', 'N/A'),\n",
    "        'company': experience.get('companyName', 'N/A'),\n",
    "        'dateRange': str(experience.get('timePeriod', {}).get('startDate', {}).get('year', 'N/A'))+' - '+ str(experience.get('timePeriod', {}).get('endDate', {}).get('year', 'Present')),\n",
    "        'location': experience.get('geoLocationName', 'N/A'),\n",
    "        'description': experience.get('description', 'N/A'),\n",
    "    }\n",
    "    newprof['experiences']+=[newExp]\n",
    "newprof['education'] = []\n",
    "for ed in profile['education']:\n",
    "    newEd = {\n",
    "        'school': ed.get('school', {}).get('schoolName', 'N/A'),\n",
    "        'degree': ed.get('degreeName', 'N/A'),\n",
    "        'dateRange': str(ed.get('timePeriod', {}).get('startDate', {}).get('year', 'N/A'))+' - '+ str(ed.get('timePeriod', {}).get('endDate', {}).get('year', 'Present')),\n",
    "        'description': '\\n'.join((ed.get('activities', ''), ed.get('description', ''))).strip(),\n",
    "    }\n",
    "    newprof['education']+=[newExp]\n",
    "\n",
    "pprint(newprof)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
