{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make necessary requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pprint import pprint\n",
    "import json\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.firefox.options import Options as FirefoxOptions\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup initial variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://www.linkedin.com/uas/login-submit\"\n",
    "redirectURL = \"Place the profile URL\"\n",
    "userEmail = \"Place your email here\"\n",
    "page404 = \"https://www.linkedin.com/jsnfkljn/?_l=en_US\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get login page and csrf value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webSession = requests.Session()\n",
    "headers = {\n",
    "\"User-Agent\":\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/110.0\"\n",
    "}\n",
    "webSession.headers.update(headers)\n",
    "loginPage = webSession.post(baseURL)\n",
    "loginSoup = bs(loginPage.content, \"html.parser\")\n",
    "# pprint(soup.prettify())\n",
    "\n",
    "csrf = (loginSoup.find('input', attrs={'name': 'loginCsrfParam'}))['value']\n",
    "pprint(csrf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the csrf and username and password to make an authenticated POST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload_dict = {'session_key':userEmail,'session_password': 'Enter your password here','loginCsrfParam': csrf,'session_redirect': redirectURL}\n",
    "loginSuccessPage = webSession.post(baseURL, data=payload_dict)\n",
    "\n",
    "loginSuccessSoup = bs(loginSuccessPage.content, \"html.parser\")\n",
    "pprint(payload_dict)\n",
    "webSession.get(redirectURL)\n",
    "# pprint(loginSuccessSoup.text)\n",
    "pprint(webSession.cookies.get_dict())\n",
    "\n",
    "# pprint(profileSoup.prettify())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 16.9kB [00:00, 1.69MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# End existing sessions\n",
    "try:\n",
    "    driver.quit()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# options = ChromeOptions()\n",
    "options = FirefoxOptions()\n",
    "options.add_argument('--lang=en')\n",
    "# options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "# service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "service = FirefoxService(executable_path=GeckoDriverManager().install())\n",
    "# driver = webdriver.Chrome(service=service)\n",
    "driver = webdriver.Firefox(service=service, options=options)\n",
    "# driver.close()\n",
    "# driver.implicitly_wait(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer session over to Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(page404)\n",
    "authCookies = ['li_at', 'liap', 'leadgen.api_session', 'transaction_state', 'lihc_auth_str',\\\n",
    "    'lihc_auth_', 'li_a', 'li_ep_auth_context', 'sj_sessionid', 'cp_session', 'G_AUTHUSER_H',\\\n",
    "        'G_ENABLED_IDPS', 'g_state', 'x-ms-cpim-cache|<id>', 'x-ms-cpim-trans',\\\n",
    "        'x-ms-cpim-sso:linkedinenterprise.onmicrosoft.com_0', 'lidc', 'li_rm']\n",
    "\n",
    "reqCookies = webSession.cookies.get_dict()\n",
    "# pprint(reqCookies)\n",
    "for cookie in reqCookies:\n",
    "    # if(cookie not in authCookies): continue\n",
    "    # pprint({\n",
    "    #     'name': cookie, \n",
    "    #     'value': reqCookies[cookie],\n",
    "    # })\n",
    "    driver.add_cookie({\n",
    "        'name': cookie, \n",
    "        'value': reqCookies[cookie],\n",
    "    })\n",
    "\n",
    "## Copy paste cookies from browser\n",
    "    \n",
    "# pprint(driver.get_cookies())\n",
    "# WebDriverWait(driver,5).until(EC.presence_of_element_located(By.XPATH('//button[text()=\"Some text\"]')))\n",
    "WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.XPATH, \"//*[contains(text(),'Page not found')]\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scape the profile!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the profile into the driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sc/trpp015x7_n6p177sy443p7h0000gn/T/ipykernel_25300/3358298637.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mredirectURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mWebDriverWait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument_initialised\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# driver.switch_to.new_window('tab');\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/New_env/lib/python3.8/site-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \n"
     ]
    }
   ],
   "source": [
    "def document_initialised(driver):\n",
    "    soup = bs(driver.page_source, 'html.parser')\n",
    "    # pprint(soup.prettify())\n",
    "    try:\n",
    "        profileSection = soup.find('div', {'class':'mt2 relative'})\n",
    "        aboutSection = soup.find('div', {'id':'about'})\n",
    "        # pprint(profileSection)\n",
    "        if(profileSection==None or aboutSection==None):\n",
    "            raise Exception('None')\n",
    "        # profileSection = profileSection['pv-text-details__left-panel']\n",
    "    \n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "driver.get(redirectURL)\n",
    "WebDriverWait(driver, timeout=5).until(document_initialised)\n",
    "# driver.switch_to.new_window('tab');\n",
    "# try:\n",
    "# except:\n",
    "#     pass\n",
    "    # driver.get(redirectURL)\n",
    "profileSoup = bs(driver.page_source, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profileScraped={\n",
    "    'name':\"\",\n",
    "    'titleDescription':\"\",\n",
    "    'location': \"\",\n",
    "    'about': \"\",\n",
    "    'experiences': \"\",\n",
    "    'education': \"\",\n",
    "    'recommendations':\"\",\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the fist ember (always the profile)\n",
    "# Go down the path for the profile name\n",
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .h1\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "profileScraped['name'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape the Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Atlanta, Georgia, United States\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .find_all('div', recursive=False)[1]\\\n",
    "        .span\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "print(res)\n",
    "profileScraped['location'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the Title's Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the fist ember (always the profile)\n",
    "# Go down the path for the profile name\n",
    "try:\n",
    "    res = profileSoup.find('section', {'id': re.compile('ember.*')})\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .find('div', {'class':'mt2 relative'})\\\n",
    "        .div\\\n",
    "        .find_all('div', recursive=False)[1]\\\n",
    "        .text\\\n",
    "        .strip()\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "profileScraped['titleDescription'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape the About section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to the ember with a child with id='about'\n",
    "# Go down the path for the description\n",
    "def aboutFilter(elem):\n",
    "    # if(elem.find('div',{'id': 'about'})): pprint(elem)\n",
    "    try:\n",
    "        return elem.name=='section' and elem.find('div',{'id': 'about'})\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "try:\n",
    "    res = profileSoup.find(aboutFilter)\\\n",
    "        .find('div', {'class':'ph5'})\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .div\\\n",
    "        .span\\\n",
    "        .text\n",
    "except:\n",
    "    res = 'N/A'\n",
    "\n",
    "profileScraped['about'] = res\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Experience"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to the 'Experience' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def experience_initialised(driver):\n",
    "#     driver.\n",
    "driver.get(redirectURL+'details/experience/')\n",
    "WebDriverWait(driver, timeout=5).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\"))\n",
    "expSoup = bs(driver.page_source, 'html.parser')\n",
    "expSoup = expSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Experience!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'company': 'College of Computing at Georgia Tech',\n",
      "  'dateRange': 'Jan 2022 - Present · 1 yr 3 mos',\n",
      "  'description': '• Held weekly office hours to tutor students on course '\n",
      "                 'material and help them prepare for exams• Facilitated '\n",
      "                 \"lecture sessions by answering students' questions or \"\n",
      "                 \"directing the questions to the lecturer • Graded students' \"\n",
      "                 'group projects and exams in limited time with individual '\n",
      "                 'comments for each mistake/correctness• Held weekly office '\n",
      "                 'hours to tutor students on course material and help them '\n",
      "                 'prepare for exams\\n'\n",
      "                 \"• Facilitated lecture sessions by answering students' \"\n",
      "                 'questions or directing the questions to the lecturer \\n'\n",
      "                 \"• Graded students' group projects and exams in limited time \"\n",
      "                 'with individual comments for each mistake/correctness',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Undergraduate Introduction to Database Systems Teaching '\n",
      "              'Assistant',\n",
      "  'type': 'Part-time · 2 yrs 3 mos'},\n",
      " {'company': 'College of Computing at Georgia Tech',\n",
      "  'dateRange': 'Jan 2021 - Dec 2021 · 1 yr',\n",
      "  'description': '• Conducted  weekly seventy-five-minute recitations to a '\n",
      "                 'section of fifty students• Held weekly office hours to tutor '\n",
      "                 'students on course material and help them prepare for tests '\n",
      "                 'and exams• Conducted  weekly seventy-five-minute recitations '\n",
      "                 'to a section of fifty students\\n'\n",
      "                 '• Held weekly office hours to tutor students on course '\n",
      "                 'material and help them prepare for tests and exams',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Undergraduate Data Structures and Algorithms Teaching Assistant',\n",
      "  'type': 'Part-time · 2 yrs 3 mos'},\n",
      " {'company': 'NCR Corporation ',\n",
      "  'dateRange': 'May 2022 - Aug 2022 · 4 mos',\n",
      "  'description': '• Developed REST APIs using Go to integrate the POS of cash '\n",
      "                 'office application with BigQuery and MongoDB databases• '\n",
      "                 'Implemented a dataflow pipeline using Terraform to transfer '\n",
      "                 'google pub-sub messages to Google BigQuery• Setup GitHub '\n",
      "                 'actions to automatically compile and deploy these dataflow '\n",
      "                 'templates to GCP storage buckets',\n",
      "  'job': ' Internship',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Software Engineering Intern'},\n",
      " {'company': 'Georgia Tech Student Foundation ',\n",
      "  'dateRange': 'Aug 2021 - May 2022 · 10 mos',\n",
      "  'description': '• Built an Automated Discounted Cash Flow tool to increase '\n",
      "                 'the efficiency of company valuation calculations',\n",
      "  'job': ' Part-time',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Quantitative Sector Analyst'},\n",
      " {'company': 'Georgia Tech VIP Program',\n",
      "  'dateRange': 'Aug 2020 - Dec 2021 · 1 yr 5 mos',\n",
      "  'description': 'RoboSense - Vertically Integrated Project• Optimized a '\n",
      "                 'multi-agent algorithm that searches for the minimum of a '\n",
      "                 'given function to arrive at less than 5e^2m from the global '\n",
      "                 'minimum on more than eight multi-dimensional functions • '\n",
      "                 'Implemented the Contention-Resolving Model Predictive '\n",
      "                 'Control(MPC) algorithm for coordinating autonomous vehicles '\n",
      "                 'at a traffic intersection using Lyft’s self-driving vehicle '\n",
      "                 'dataset',\n",
      "  'job': 'N/A',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Undergraduate Student Researcher'},\n",
      " {'company': 'Bashpole Software, Inc. ',\n",
      "  'dateRange': 'May 2021 - Aug 2021 · 4 mos',\n",
      "  'description': '• Developed a full stack application using JSP that '\n",
      "                 'generates optimal landing pages for Bashpole’s clients• '\n",
      "                 'Refactored the template code base to integrate all '\n",
      "                 'individual Java landing pages into a single Maven project to '\n",
      "                 'aid with project setup, test code, and have code uniformity, '\n",
      "                 'which reduced the onboarding period by 67%',\n",
      "  'job': ' Internship',\n",
      "  'location': 'N/A',\n",
      "  'position': 'Software Engineering Intern'},\n",
      " {'company': 'Georgia Institute of Technology',\n",
      "  'dateRange': 'Dec 2020 - May 2021 · 6 mos',\n",
      "  'description': '• Created a platform to administer and visualize patient’s '\n",
      "                 'progress during PTSD therapy• Built the Prolonged Exposure '\n",
      "                 'Collective Sensing System(PECCS) android app that helps '\n",
      "                 'doctors better analyze and understand how patients behave in '\n",
      "                 'their absence, using data collected using patient’s homework '\n",
      "                 'sessions',\n",
      "  'job': 'N/A',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Undergraduate Research Assistant'},\n",
      " {'company': 'Georgia Tech Student Foundation',\n",
      "  'dateRange': 'Aug 2020 - May 2021 · 10 mos',\n",
      "  'description': '• Researched Energy sector companies for presentations by '\n",
      "                 'using 10-Ks/10-Qs/8-Ks, sell-side research reports and '\n",
      "                 'Bloomberg Terminal• Presented energy sector and '\n",
      "                 'macroeconomic research to the committee on prospective '\n",
      "                 'holdings and reevaluation of current holdings',\n",
      "  'job': 'N/A',\n",
      "  'location': 'Atlanta, Georgia, United States',\n",
      "  'position': 'Energy Sector Analyst'},\n",
      " {'company': 'General Motors East Africa Limited Nairobi ',\n",
      "  'dateRange': 'Jun 2017 - Jul 2017 · 2 mos',\n",
      "  'description': '•\\tAnalyzed CAD-based design of truck body parts, and '\n",
      "                 'studied robots that carried out the automated heavy-vehicle '\n",
      "                 'assembly•\\tCoordinated with employees of the Manufacturing '\n",
      "                 'and Engineering departments',\n",
      "  'job': ' Internship',\n",
      "  'location': 'N/A',\n",
      "  'position': 'Shadowing Intern'}]\n"
     ]
    }
   ],
   "source": [
    "experience = []\n",
    "pastExperience = []\n",
    "try:\n",
    "    pastExperience = expSoup.find_all('li', recursive=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for exp in pastExperience:\n",
    "    job = {}\n",
    "    subjobs=[]\n",
    "    try:\n",
    "        subjobs = exp.div.div.find_all('div', recursive=False)[1].find_all('div', recursive=False)[1].ul.li.div.div.div.ul.find_all('li', recursive=False)\n",
    "    except:\n",
    "        pass\n",
    "    # pprint(subjobs[0])\n",
    "    if(len(subjobs)!=0):\n",
    "        try:\n",
    "            job['company'] = exp.div\\\n",
    "                        .div\\\n",
    "                        .find_all('div', recursive=False)[1]\\\n",
    "                        .div\\\n",
    "                        .a\\\n",
    "                        .div\\\n",
    "                        .span\\\n",
    "                        .span\\\n",
    "                        .text\n",
    "        except:\n",
    "            job['company'] = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            job['type'] = exp.div\\\n",
    "                        .div\\\n",
    "                        .find_all('div', recursive=False)[1]\\\n",
    "                        .div\\\n",
    "                        .a\\\n",
    "                        .find('span', recursive=False)\\\n",
    "                        .span\\\n",
    "                        .text\n",
    "        except:\n",
    "            job['type'] = 'N/A'\n",
    "        \n",
    "        try:\n",
    "            job['location'] = exp.div\\\n",
    "                        .div\\\n",
    "                        .find_all('div', recursive=False)[1]\\\n",
    "                        .div\\\n",
    "                        .a\\\n",
    "                        .find_all('span', recursive=False)[1]\\\n",
    "                        .span\\\n",
    "                        .text\n",
    "        except:\n",
    "            job['location'] = 'N/A'\n",
    "        \n",
    "        for sj in subjobs:\n",
    "            try:\n",
    "                job['position'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .div\\\n",
    "                    .a\\\n",
    "                    .div\\\n",
    "                    .span\\\n",
    "                    .span\\\n",
    "                    .text\n",
    "            except:\n",
    "                job['position'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['dateRange'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .div\\\n",
    "                    .a\\\n",
    "                    .find('span', recursive=False)\\\n",
    "                    .span\\\n",
    "                    .text\n",
    "            except:\n",
    "                job['dateRange'] = 'N/A'\n",
    "            \n",
    "            try:\n",
    "                job['description'] = sj.div\\\n",
    "                    .div\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .find_all('div', recursive=False)[1]\\\n",
    "                    .ul\\\n",
    "                    .li\\\n",
    "                    .text\\\n",
    "                    .strip()\n",
    "            except:\n",
    "                job['description'] = 'N/A'\n",
    "                \n",
    "            experience+=[copy.deepcopy(job)]\n",
    "\n",
    "        continue\n",
    "            \n",
    "        \n",
    "    try:\n",
    "        job['position'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['position'] = 'N/A'\n",
    "    try:\n",
    "        job['company'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find('span', recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split('·')[0]\n",
    "    except:\n",
    "        job['company'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        job['job'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find('span', recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split('·')[1]\n",
    "    except:\n",
    "        job['job'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        job['dateRange'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find_all('span', recursive=False)[1]\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['dateRange'] = 'N/A'\n",
    "            \n",
    "    try:\n",
    "        job['location'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .find_all('span', recursive=False)[2]\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['location'] = 'N/A'\n",
    "\n",
    "    try:\n",
    "        job['description'] = exp.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .ul\\\n",
    "            .li\\\n",
    "            .div\\\n",
    "            .ul\\\n",
    "            .li\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        job['description'] = 'N/A'\n",
    "            \n",
    "    experience+=[job]\n",
    "pprint(experience)\n",
    "profileScraped['experience'] = experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Education"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to the 'Education' page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(redirectURL+'details/education/')\n",
    "WebDriverWait(driver, timeout=7).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\") and d.find_element(By.CLASS_NAME,\"ivm-image-view-model\"))\n",
    "edSoup = bs(driver.page_source, 'html.parser')\n",
    "edSoup = edSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Education!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education = []\n",
    "pastEducation = []\n",
    "try:\n",
    "    pastEducation = edSoup.find_all('li', recursive=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for current in pastEducation:\n",
    "    school = {}\n",
    "    try:\n",
    "        school['name'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        school['name'] = 'N/A'\n",
    "    try:\n",
    "        school['degree'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span',class_=\"t-14 t-normal\", recursive=False)\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        school['degree'] = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        school['dateRange'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span', {'class': 't-black--light'}, recursive=False)\\\n",
    "            .span\\\n",
    "            .text\n",
    "        \n",
    "    except:\n",
    "        school['dateRange'] = 'N/A'\n",
    "        \n",
    "    try:\n",
    "        school['description'] = current.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .text\\\n",
    "        \n",
    "        school['description'] = (re.sub(r'\\n\\s*\\n', '\\n\\n', school['description'])).strip()\n",
    "        \n",
    "    except:\n",
    "        school['description'] = 'N/A'\n",
    "    \n",
    "    education+=[school]\n",
    "\n",
    "print(education)\n",
    "profileScraped['education'] = education"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape Recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Navigate to Recommendations page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(redirectURL+'details/recommendations/')\n",
    "WebDriverWait(driver, timeout=7).until(lambda d: d.find_element(By.TAG_NAME,\"ul\") and d.find_element(By.CLASS_NAME,\"pvs-list\") and d.find_element(By.CLASS_NAME,\"t-black--light\"))\n",
    "recSoup = bs(driver.page_source, 'html.parser')\n",
    "recSoup = recSoup.find('ul', {'class':'pvs-list'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally Scrape Recommendations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = []\n",
    "pastRecommendations = recSoup.find_all('li', recursive=False)\n",
    "\n",
    "for rec in pastRecommendations:\n",
    "    recLi = {}\n",
    "    try:\n",
    "        recLi['name'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .div\\\n",
    "            .span\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        recLi['name'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        recLi['profile'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .get('href')\n",
    "    except:\n",
    "        recLi['profile'] = 'N/A'\n",
    "\n",
    "    try:\n",
    "        subtitle = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .div\\\n",
    "            .a\\\n",
    "            .find('span', attrs = {'class': 't-black--light'}, recursive=False)\\\n",
    "            .span\\\n",
    "            .text\\\n",
    "            .split()\n",
    "        \n",
    "        recLi['date'] = ' '.join(subtitle[:3])\n",
    "        recLi['relationship'] = ' '.join(subtitle[3:])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        recLi['date'] = 'N/A'\n",
    "        recLi['relationship'] = 'N/A'\n",
    "    \n",
    "    try:\n",
    "        recLi['testimonial'] = rec.div\\\n",
    "            .div\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find_all('div', recursive=False)[1]\\\n",
    "            .find('div', attrs={'class': 't-normal'})\\\n",
    "            .span\\\n",
    "            .text\n",
    "    except:\n",
    "        recLi['testimonial'] = 'N/A'\n",
    "    \n",
    "    recommendations+=[recLi]\n",
    "\n",
    "pprint(recommendations)\n",
    "profileScraped['recommendations'] = recommendations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import Person\n",
    "person = Person(\"https://www.linkedin.com/in/aravindvengarai/\", driver=driver)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
